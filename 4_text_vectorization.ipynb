{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    " ## Text vectorisation\n",
    " \n",
    " Raw textual data is untractable for statistical learning algorithms which can generally\n",
    " handle only numeric data. The process process of mapping a document (text) to a numeric space is\n",
    " called feature extraction or vectorization and is crucial for the performance of statistical learning\n",
    " methods.\n",
    " \n",
    " For the purposes of text classification the vectorization process should map similar (in meaning) \n",
    " documents to points that are close to each other in the numeric space (semantic space).   \n",
    " \n",
    " ### Bag-of-words \n",
    " \n",
    " Consider a collection (corpus) consisting of $$D$$ documents. Denote the number of unique words (vocabulary)\n",
    " in that collection be indexed by $$j=1,\\ldots,N$$.\n",
    " \n",
    " A simple yet commonly used model is the bag-of-words model that maps documents into\n",
    " a space of dimension $$N$$ spanned by the set of unique words (n-grams) in the vocabulary."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "['are',\n 'bad',\n 'good',\n 'hate',\n 'he',\n 'is',\n 'love',\n 'mary',\n 'they',\n 'us',\n 'we',\n 'you']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 45
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "positive_texts = [\n",
    "    \"we love you\",\n",
    "    \"they love us\",\n",
    "    \"you are good\",\n",
    "    \"he is good\",\n",
    "    \"they love mary\"\n",
    "]\n",
    "\n",
    "negative_texts = [\n",
    "    \"we hate you\",\n",
    "    \"they hate us\",\n",
    "    \"you are bad\",\n",
    "    \"he is bad\",\n",
    "    \"we hate mary\"\n",
    "]\n",
    "\n",
    "all_texts = positive_texts + negative_texts\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vectorizer.fit(all_texts)\n",
    "vectorizer.get_feature_names()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    During fitting the vectorizer builds the collection vocabulary."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "{'are': 0,\n 'bad': 1,\n 'good': 2,\n 'hate': 3,\n 'he': 4,\n 'is': 5,\n 'love': 6,\n 'mary': 7,\n 'they': 8,\n 'us': 9,\n 'we': 10,\n 'you': 11}"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 46
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    After the vectorizer is fitted we can\n",
    "    transform the text to obtain frequency count vectors.\n",
    "    \n",
    "    The resulting document-word matrix has only a few non-zero\n",
    "    entries and a large amount of zeroes (sparse matrix)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1],\n       [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0],\n       [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1],\n       [0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0],\n       [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0]])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 47
    }
   ],
   "source": [
    "texts_transformed = vectorizer.transform(all_texts)\n",
    "texts_transformed.toarray()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    The count vectorizer from `scikit-learn` produces a matrix where each row corresponds\n",
    "    to a document and each entry $$i,j$$ contains the frequency of vocabulary word $$j$$ within document $$j$$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    The bag-of-words model disregards the word order and the grammaer and count vectors may be quite unbalanced \n",
    "    with the more common words having much higher frequencies than the less common ones (Zipf's law).\n",
    "    This may have an impact on some models, e.g. generalized linear models.\n",
    "    \n",
    "    Another way to vectorize the documents in the collection is to truncate the frequencies at 1. The result is\n",
    "    called one-hot encoding."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1],\n       [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0],\n       [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1],\n       [0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0],\n       [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0]])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 48
    }
   ],
   "source": [
    "one_hot_vectorizer = Binarizer()\n",
    "\n",
    "one_hot_encoded_vecs = one_hot_vectorizer.fit_transform(texts_transformed)\n",
    "one_hot_encoded_vecs.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    The bag-of-word representation of documents treats each document in isolation, without considering\n",
    "    the whole collection. A way to introduce the context of the collection is to weight the \n",
    "    frequencies so that words that occur frequently in the document but are not frequently found in other\n",
    "    documents receive a higher weight. One way to vectorization that achieve this\n",
    "    is to use the inverse document frequency.\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.59863623, 0.        , 0.        , 0.        ,\n        0.59863623, 0.53223051],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.5499426 , 0.        , 0.5499426 , 0.62859071,\n        0.        , 0.        ],\n       [0.6195754 , 0.        , 0.6195754 , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.48192597],\n       [0.        , 0.        , 0.57735027, 0.        , 0.57735027,\n        0.57735027, 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.5499426 , 0.62859071, 0.5499426 , 0.        ,\n        0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.59863623, 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.59863623, 0.53223051],\n       [0.        , 0.        , 0.        , 0.5499426 , 0.        ,\n        0.        , 0.        , 0.        , 0.5499426 , 0.62859071,\n        0.        , 0.        ],\n       [0.6195754 , 0.6195754 , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.48192597],\n       [0.        , 0.57735027, 0.        , 0.        , 0.57735027,\n        0.57735027, 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.5499426 , 0.        ,\n        0.        , 0.        , 0.62859071, 0.        , 0.        ,\n        0.5499426 , 0.        ]])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 49
    }
   ],
   "source": [
    "idf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "texts_idf_vecs = idf_vectorizer.fit_transform(all_texts)\n",
    "texts_idf_vecs.toarray()\n",
    "\n",
    "## \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "   ## Reducing the dimenisons of the feature space\n",
    "   \n",
    "   When dealing with large collections with long documents and high number of unique words the\n",
    "   dimension of the feature space can become high. \n",
    "   \n",
    "   One way to reduce the dimensionality of the feature space while retaining _most_ of the information\n",
    "   is to use principal components analysis (PCA).  "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.23606781, 0.42469203, 0.56094171, 0.660828  , 0.74051767,\n       0.81415114])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 54
    }
   ],
   "source": [
    "\n",
    "pca = PCA(6)\n",
    "\n",
    "reduced = pca.fit_transform(texts_idf_vecs.toarray())\n",
    "reduced\n",
    "\n",
    "variance_explained = np.cumsum(pca.explained_variance_)\n",
    "variance_explained"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}